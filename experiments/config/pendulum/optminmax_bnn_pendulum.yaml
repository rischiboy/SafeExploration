wandb:
  project_name: "Final"                                         # Name of the project on wandb
  group_name: "Unsafe-CEM"                                          # Name of the run on wandb
  job_name: "Opt-Unsafe"                                 # Name of the job
  logs_dir: "./experiments/results/pendulum"              # Directory to save the logs
  logging_wandb: True                                     # Whether to log the training to wandb
env:
  action_cost: 0.001                  # Default: 0.001
  angle_tolerance: 0.1
  stability_duration: 10
  max_steps: 200
  max_speed: 12.0
constraint:
  speed_threshold: 12.0
  lmbda: 1.0
  d: 0.0
  barrierfn: "relu"                 # ["relu", "huber", "quadratic", "exponential"]
minmax:
  horizon_x: 20                     # Planning horizon
  num_iter_x: 10                     # Number of consecutive CEM update iterations
  num_fixed_elites_x: 5             # Number of elite samples considered to evaluate the joint objective function     
  num_elites_x: 100                  # Number of elite samples considered to update the distribution
  num_samples_x: 1000                # Total number of samples
  # exponent_x: 2.0                   # Specificially for the ICEM Agent
  horizon_y: 20
  num_iter_y: 0
  num_fixed_elites_y: 5
  num_elites_y: 50
  num_samples_y: 500
  # exponent_y: 2.0
  pes_alpha: 1.0                    # Hyperparameter to define the pessimistic exploration
  opt_alpha: 1.0                    # Hyperparameter to define the optimistic exploration
  iterations: 1                    # Number of MinMax iterations
model:
  dynamics: "BNN"                   # ["TRUE", "BNN", "GP"]
  optimizer: "MinMax"               # ["CEM", "ICEM" "MinMax"]
  agent: "OptMinMax"                # ["CEM", "SafeCEM", "PesTraj" "MinMax", "OptMinMax"]
  bnn_type: "DeterministicFSVGDEnsemble" # ["DeterministicEnsemble", "ProbabilisticEnsemble", "DeterministicFSVGDEnsemble", "ProbabilisticFSVGDEnsemble"]
  sampling_mode: "DIST"             # ["MEAN", "NOISY_MEAN", "DIST", "TS"]
  output_stds: [0.0001, 0.0001, 0.0001]    # Standard deviation of the output of the model -- ProbabilisticEnsemble
  num_training_steps: 10            # Number of training epochs to fit the BNN model -- BNNStatisticalModel
  beta: [1.0, 1.0, 1.0]             # Constant scheduler -- BNNStatisticalModel
  features: [128, 128]              # Number of hidden units in the MLP
  lr_rate: 0.001                   # Learning rate for the Adam optimizer
  weight_decay: 0.0001               # Weight decay for Adam optimizer
  num_calibration_ps: 20            # Number of calibration bins
  num_test_alphas: 100              # Number of calibration candidates
  num_ensembles: 5                  # Number of ensembles for the ensemble model
  num_particles: 1                  # Number of particles to sample from the distribution    
  train_share: 0.8                  # Share of the data used for training
  batch_size: 64                    # Batch size for training the BNN model
  eval_frequency: 5                 # Number of training epochs between evaluation of the model
  eval_batch_size: 2048             # Batch size for evaluating the model
  return_best_model: False          # Whether to return the best model parameters based on the validation loss
train:
  # seed: 0
  sample_batch_size: 256            # Batch size for sampling from the replay buffer
  buffer_size: 30000                # Size of the replay buffer
  num_model_updates: 50            # Number of training iterations
  num_rollout_steps: 200            # Number of steps to rollout the agent
  num_exploration_steps: 100        # Number of steps to collect data for the replay buffer
  val_buffer_size: 500              # Size of the validation buffer
  val_batch_size: 500               # Batch size for evaluating the model
  eval_episodes: 0                  # Number of episodes to evaluate the agent
  eval_model_freq: 0                # Number of training iterations between evaluation
  plot_freq: 10                     # Number of training iterations between plotting
  diff_states: True                # Whether to use the difference between states as input  
  verbose: True                     # Whether to print the training progress  
  render: True                      # Whether to store recordings of the evaluation episodes
  calibrate: True                   # Whether to calibrate the model