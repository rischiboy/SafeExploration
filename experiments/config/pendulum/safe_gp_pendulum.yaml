wandb:
  project_name: "Week-23"                                         # Name of the project on wandb
  group_name: "GP"                                            # Name of the run on wandb
  job_name: "Safe"                                      # Name of the job
  logs_dir: "./experiments/results/pendulum"                      # Directory to save the logs
  logging_wandb: True                                             # Whether to log the training to wandb
env:
  angle_tolerance: 0.1
  stability_duration: 10
  max_steps: 200
  max_speed: 12.0
constraint:
  speed_threshold: 6.0
  lmbda: 100.0
  d: 0.0
icem:
  horizon: 20                       # Planning horizon
  num_iter: 10                      # Number of consecutive CEM update iterations        
  num_elites: 50                    # Number of elite samples considered to update the distribution
  num_samples: 500                  # Total number of samples
  exponent: 2.0                     # Specificially for the ICEM Agent
model:
  dynamics: "GP"                    # ["TRUE", "BNN", "GP"]
  optimizer: "ICEM"                 # ["CEM", "ICEM" "MinMax"]
  agent: "SafeCEM"                  # ["CEM", "SafeCEM", "PesTraj" "MinMax"]
  sampling_mode: "DIST"             # ["MEAN", "NOISY_MEAN", "DIST", "TS"]
  # f_norm_bound: 1.0               # Bound for the Frobenius norm of the kernel matrix
  # delta: 0.1                      # Confidence level for the GP model
  num_training_steps: 5             # Number of training epochs to fit the model -- StatisticalModel
  beta: [1.0, 1.0, 1.0]             # Constant scheduler -- StatisticalModel
  output_stds: [0.005, 0.005, 0.05]        # Standard deviation of the output of the model -- ProbabilisticEnsemble
  # kernel: "RBF"                   # Kernel function for the GP model
  weight_decay: 0.001               # Weight decay for Adam optimizer
  lr_rate: 0.0001                   # Learning rate for the Adam optimizer
  num_particles: 1                  # Number of particles to sample from the distribution
train:
  # seed: 11932
  sample_batch_size: 256            # Batch size for sampling from the replay buffer
  buffer_size: 30000                # Size of the replay buffer
  num_model_updates: 30            # Number of training iterations
  num_rollout_steps: 200            # Number of steps to rollout the agent
  num_exploration_steps: 200       # Number of steps to collect data for the replay buffer
  val_buffer_size: 500              # Size of the validation buffer
  val_batch_size: 500               # Batch size for evaluating the model
  eval_episodes: 0                  # Number of episodes to evaluate the agent
  eval_model_freq: 0                # Number of training iterations between evaluation
  diff_states: True                # Whether to use the difference between states as input  
  verbose: True                     # Whether to print the training progress  
  render: True                      # Whether to store recordings of the evaluation episodes