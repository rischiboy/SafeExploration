import time
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import jax
import jax.numpy as jnp
from tqdm import tqdm
from typing import Callable, Dict, List, NamedTuple, Tuple, Union
from stochastic_optimization.environment.pendulum_env import ConstrainedPendulum
from stochastic_optimization.dynamical_system.pendulum_system import (
    PendulumCost,
    PendulumDynamicsParams,
    PendulumSystem,
    SafePendulumSystem,
)
from stochastic_optimization.optimizer.cem_planner import CrossEntropyMethod, CEMPlanner
from stochastic_optimization.optimizer.min_max import (
    MinMaxOptimizer,
    OptVarConstants,
    OptVarParams,
)
from stochastic_optimization.optimizer.min_max_planner import (
    MinMaxPlanner,
    simulate_trajectory,
)
from stochastic_optimization.optimizer.safe_cem_planner import SafeCEMPlanner
from stochastic_optimization.optimizer.utils import (
    mean_reward,
    relu_augmented_lagragian,
)

global cem_results

"""
Store path to working directory
"""
file_path = os.path.abspath(__file__)
cwd = os.path.dirname(file_path)

plot_dir = os.path.join(cwd, "plots")
plot_prefix = "violation-plot"

data_dir = os.path.join(cwd, "data")
data_prefix = "violation-data"

plot_comparison_prefix = "violation-comparison-plot"
data_comparison_prefix = "violation-comparison-data"

plot_parameters_prefix = "violation-parameters-plot"

plot_max_speed_prefix = "max-speed-plot"

""" Setup class """


class Setup(NamedTuple):
    env: ConstrainedPendulum
    policy: Callable
    policy_name: str


"""
Policy functions
"""


def max_policy(obs: np.ndarray):
    action = env.action_space.high
    return action


def random_policy(obs: np.ndarray):
    action = env.action_space.sample()
    return action


def cem_policy(
    obs: np.ndarray, optimizer: CEMPlanner, optimize_fn: Callable, rng: jnp.ndarray
):
    model_params, reward_params = optimizer.dynamical_system.init(key=rng)
    rng, eval_key, optimizer_key = jax.random.split(rng, num=3)
    seq, costs = optimizer.optimize_trajectory(
        optimize_fn, obs, model_params, eval_key, optimizer_key
    )
    action = seq[0]
    return action


def safe_cem_policy(
    obs: np.ndarray, optimizer: SafeCEMPlanner, optimize_fn: Callable, rng: jnp.ndarray
):
    model_params, reward_params, cost_params = optimizer.dynamical_system.init(key=rng)
    rng, eval_key, optimizer_key = jax.random.split(rng, num=3)
    seq, costs = optimizer.optimize_trajectory(
        optimize_fn, obs, model_params, eval_key, optimizer_key
    )
    action = seq[0]
    return action


def min_max_policy(
    obs: np.ndarray, optimizer: MinMaxPlanner, optimize_fn: Callable, rng: jnp.ndarray
):
    model_params, reward_params, cost_params = optimizer.dynamical_system.init(key=rng)
    rng, eval_key, optimizer_key = jax.random.split(rng, num=3)
    seq, hal_seq = optimizer.optimize_trajectory(
        optimize_fn, obs, model_params, eval_key, optimizer_key
    )
    check_min_max_trajectory(
        optimizer.dynamical_system, model_params, obs, seq, hal_seq, eval_key
    )
    action = seq[0]
    return action


"""
Tests and Checks
"""


"""
Checks whether the optmistic and pessimistic trajectories generated by the MinMaxPlanner are the same.
"""


def check_min_max_trajectory(
    system: SafePendulumSystem,
    model_params: PendulumDynamicsParams,
    init_obs: np.ndarray,
    action_seq: np.ndarray,
    hal_action_seq: np.ndarray,
    eval_key: jnp.ndarray,
):
    opt_trajectory, pes_trajectory, rewards, costs = simulate_trajectory(
        system, model_params, init_obs, action_seq, hal_action_seq, alpha, eval_key
    )

    assert len(opt_trajectory) == len(
        pes_trajectory
    ), "Trajectories must have the same length"
    assert np.all(opt_trajectory == pes_trajectory), "Trajectories must be the same"


"""
Checks whether the constraint is violated at any point in the trajectory.
"""


def check_constraint(
    env,
    policy: Callable,
    constraint: Callable,
    state: np.ndarray = None,
    num_steps: int = 200,
    key: jnp.ndarray = None,
):
    if key is None:
        assert seed is not None, "Seed must be provided if key is None"
        key = jax.random.PRNGKey(seed)

    violated = False

    if state is not None:
        obs, _ = env.reset_any(state=state)
    else:
        obs, _ = env.reset()

    transition_df = pd.DataFrame(
        columns=["step", "obs", "action", "next_obs", "reward"]
    )
    # tqdm.write(f"Initial state: {obs}")

    for i in range(num_steps):
        key, eval_key = jax.random.split(key)
        action = policy(obs, eval_key)
        next_obs, reward, finished, truncated, _ = env.step(action)
        transition_df.loc[i] = [i, obs, action, next_obs, reward]

        if constraint(next_obs):
            # print(f"Step {i}: Constraint violated at {next_obs}")
            violated = True
        obs = next_obs

    return transition_df, violated, finished


"""
Runs multiple runs of the constraint check test for a given policy and constraint.
"""


def run_test(
    env, policy: Callable, key: jnp.ndarray, dry_run: bool = True, num_runs: int = 20
):
    num_violations = 0
    num_finished = 0
    for i in tqdm(range(num_runs)):
        trajectory_df, violated, finished = check_constraint(
            env, policy, constraint, num_steps=num_steps, key=key
        )

        if violated:
            largest_violation_idx = (
                trajectory_df["obs"].map(lambda x: np.abs(x[-1])).idxmax()
            )
            largest_violation = trajectory_df.loc[largest_violation_idx, "obs"][-1]
            tqdm.write(f"Run {i} - Largest violation: {largest_violation}")
            if not dry_run:
                store_data(trajectory_df, prefix=data_prefix)
                plot_trajectory(trajectory_df)
            num_violations += 1

        if finished:
            num_finished += 1

    print(
        f"Violations: {num_violations}/{num_runs} \t Finished: {num_finished}/{num_runs}"
    )

    return num_violations, num_finished


"""
Compares the constraint violation of two policies for the same initial states.
"""


def comparative_test(
    setup_list: List[Setup],
    dry_run: bool = True,
    num_runs: int = 20,
    rng: jnp.ndarray = None,
):
    def single_run(setup: Setup, state: np.ndarray, key: jnp.ndarray):
        start = time.time()

        trajectory_df, violated, finished = check_constraint(
            setup.env,
            setup.policy,
            constraint,
            state=state,
            num_steps=num_steps,
            key=key,
        )
        elapsed = time.time() - start

        if violated:
            largest_violation_idx = (
                trajectory_df["obs"].map(lambda x: np.abs(x[-1])).idxmax()
            )
            largest_violation = trajectory_df.loc[largest_violation_idx, "obs"][-1]
            num_violation_states = trajectory_df["obs"].map(constraint).sum()
            tqdm.write(
                f"({setup.policy_name}) Run {i+1} - Largest violation: {largest_violation:.3f} - Number of violated states: {num_violation_states} - Finished: {finished} - Time: {elapsed:.3f}s"
            )
        else:
            tqdm.write(
                f"({setup.policy_name}) Run {i+1} - No constraint violation - Finished: {finished} - Time: {elapsed:.3f}s"
            )

        return trajectory_df, violated, finished

    if rng is None:
        assert seed is not None, "Seed must be provided if rng is None"
        rng = jax.random.PRNGKey(seed)

    num_setups = len(setup_list)

    # Generate initial state for tests
    init_states_env = setup_list[0].env
    init_states = generate_initial_states(init_states_env, num_runs=num_runs, any=False)

    # init_states = [np.array([-3.0, 4.0])]

    num_violations = np.zeros(num_setups, dtype=int)
    num_finished = np.zeros(num_setups, dtype=int)

    for i in tqdm(range(num_runs)):
        state = init_states[i]
        rng, run_key = jax.random.split(rng)

        trajectories = []
        for j in range(num_setups):
            setup = setup_list[j]
            trajectory_df, violated, finished = single_run(setup, state, run_key)
            trajectories.append(trajectory_df)

            if violated:
                num_violations[j] += 1
            if finished:
                num_finished[j] += 1

        # Plot the trajectory data if either policy violated the constraint
        # if violated_1 or violated_2:
        if not dry_run:

            labeled_trajectories = [
                (setup.policy_name, df) for setup, df in zip(setup_list, trajectories)
            ]
            plot_file = comparison_plot(labeled_trajectories)

            trajectories = [
                df.add_suffix(f"_{setup_list[j].policy_name}")
                for j, df in enumerate(trajectories)
            ]
            merged_trajectory_df = pd.concat(trajectories, axis=1)
            data_file = store_data(merged_trajectory_df, prefix=data_comparison_prefix)

            tqdm.write(f"Plot: {plot_file} - Data: {data_file}")

    # Print experiment stats

    for j in range(num_setups):
        setup = setup_list[j]
        print(
            f"({setup.policy_name}) Violations: {num_violations[j]}/{num_runs} \t Finished: {num_finished[j]}/{num_runs}"
        )

    return


"""
Running two policies for the same initial states and comparing the constraint violation at the same time.
"""


def simultaneous_run(
    setup_list: List[Setup],
    key: jnp.ndarray,
    dry_run: bool = True,
    num_steps: int = 200,
):

    init_state = np.array([-3.0, 4.0])
    num_setups = len(setup_list)

    # Environment must be the same for all setups in the provided list
    env = setup_list[0].env
    obs, _ = env.reset_any(state=init_state)
    # obs, _ = env.reset(seed=seed)
    print(f"Initial state: {obs}")

    for i in range(num_steps):
        key, eval_key = jax.random.split(key)
        setup_actions = []
        for j in range(num_setups):
            setup = setup_list[j]
            policy = setup.policy
            action = policy(obs, eval_key)
            setup_actions.append(action)

        try:
            assert all(
                x == setup_actions[0] for x in setup_actions
            ), "All values in setup_action must be the same"
        except AssertionError as e:
            print(f"Step {i}: {setup_actions}")
            print(f"Obs: {obs}")
            raise e

        next_obs, reward, finished, truncated, _ = env.step(setup_actions[0])
        obs = next_obs

    return


"""
Tests the constraint violation for a range of parameters of the optimization function.
"""


def test_parameters(
    lmbda: Union[List[float], float],
    d: Union[List[float], float],
    dry_run: bool = True,
    num_runs: int = 50,
):
    assert isinstance(lmbda, list) or isinstance(
        lmbda, float
    ), "lmbda must be a float or list"
    assert isinstance(d, list) or isinstance(d, float), "d must be a float or list"
    assert (isinstance(lmbda, list) and isinstance(d, float)) or (
        isinstance(lmbda, float) and isinstance(d, list)
    ), "lmbda and d must be of different types"

    is_lambda = False
    if isinstance(lmbda, list):
        num_values = len(lmbda)
        is_lambda = True
    else:
        num_values = len(d)

    # Generate initial state for tests
    init_states = generate_initial_states(env, num_runs=num_runs, any=True)
    results = []
    rng = jax.random.PRNGKey(seed=seed)

    for i in tqdm(range(num_values)):
        if is_lambda:
            lmbda_i = lmbda[i]
            d_i = d
        else:
            lmbda_i = lmbda
            d_i = d[i]

        safe_cem_optimize_fn = lambda reward, cost: relu_augmented_lagragian(
            reward, cost, d=d_i, lmbda=lmbda_i
        )

        safe_policy = lambda obs: safe_cem_policy(
            obs, safe_cem_planner, safe_cem_optimize_fn, rng
        )

        num_violations = 0
        num_finished = 0
        for j in tqdm(range(num_runs), leave=False):
            _, violated, finished = check_constraint(
                env, safe_policy, constraint, state=init_states[j]
            )
            if violated:
                num_violations += 1
            if finished:
                num_finished += 1

        results.append(num_violations)
        tqdm.write(
            f"[Lambda: {lmbda_i} | d: {d_i}] - Violations: {num_violations}/{num_runs} \t Finished: {num_finished}/{num_runs}"
        )
        safe_cem_planner.reset()

    if not dry_run:
        if is_lambda:
            x_values = np.array(lmbda)
            x_label = "Lambda"
        else:
            x_values = np.array(d)
            x_label = "d"

        y_values = np.array(results)

        plt.plot(x_values, y_values)
        plt.scatter(x_values, y_values)

        plt.xlabel(x_label)
        plt.ylabel("Number of Violations")
        plt.title("Constraint Violation vs. Parameters")
        if is_lambda:
            plt.xscale("log")

        ext = ".png"
        plot_name = generate_file_name(plot_dir, plot_parameters_prefix, ext) + ext
        plot_path = os.path.join(plot_dir, plot_name)
        plt.savefig(plot_path)

    return


def solvable_horizon_test(
    horizon_list: List[int],
    default_config: Dict,
    rng: jnp.ndarray,
    dry_run: bool = True,
):
    for horizon in horizon_list:
        default_config["horizon"] = horizon
        cem_planner = create_cem_planner(default_config)
        safe_cem_planner = create_safe_planner(default_config)

        custom_cem_policy = lambda obs, rng: cem_policy(
            obs, cem_planner, cem_optimize_fn, rng
        )
        custom_safe_cem_policy = lambda obs, rng: safe_cem_policy(
            obs, safe_cem_planner, safe_cem_optimize_fn, rng
        )

        # cem_setup = Setup(env, custom_cem_policy, "CEM")
        cem_setup = Setup(constrained_env, custom_cem_policy, "CEM")
        safe_cem_setup = Setup(constrained_env, custom_safe_cem_policy, "Safe CEM")

        print("Running test for horizon: ", horizon)

        comparative_test(
            [cem_setup, safe_cem_setup],
            dry_run=dry_run,
            num_runs=1,
            rng=rng,
        )

    return


"""
Determine the maximum speed for which the environment is solvable.
"""


def test_max_speed_threshold(
    env, max_speed: List[float], dry_run: bool = True, num_runs: int = 20
):
    assert isinstance(max_speed, list), "max_speed must be a list"

    num_values = len(max_speed)
    results = []

    # Generate initial state for tests
    init_states = generate_initial_states(env, num_runs=num_runs, any=False)

    # Scale according to the smallest value of max_speed such that
    # min_speed = min(max_speed)
    # scale_speed = lambda x, mx: np.array([x[0], (x[1] / mx) * min_speed])
    # scaled_init_states = [
    #     scale_speed(state, original_max_speed) for state in init_states
    # ]
    scaled_init_states = init_states

    if not dry_run:
        fig, ax = plt.subplots(3, 1, figsize=(10, 15), gridspec_kw={"hspace": 0.5})

    data_idx = None
    for i in tqdm(range(num_values)):
        max_speed_i = max_speed[i]
        env = ConstrainedPendulum(max_torque=max_torque, max_speed=max_speed_i)
        # scaled_init_states = [scale_speed(state, max_speed_i) for state in init_states]
        constraint = lambda x: x[-1] > max_speed_i or x[-1] < -max_speed_i

        num_finished = 0
        plot_data = None
        for j in tqdm(range(num_runs), leave=False):
            # Ignore the violations, just plan
            trajectory_df, violated, finished = check_constraint(
                env, custom_cem_policy, constraint, state=scaled_init_states[j]
            )
            if i == 0:
                if plot_data is None:
                    plot_data = trajectory_df
                    data_idx = j
                if finished:
                    num_finished += 1
                else:
                    plot_data = trajectory_df
                    data_idx = j
            else:
                if j == data_idx:
                    plot_data = trajectory_df
                if finished:
                    num_finished += 1

        results.append(num_finished)
        tqdm.write(f"[Max Speed: {max_speed_i}] - Finished: {num_finished}/{num_runs}")

        if not dry_run:
            x_values = plot_data["step"].values
            angle = plot_data["obs"].map(lambda x: np.arctan2(x[1], x[0])).values
            angular_velocity = plot_data["obs"].map(lambda x: x[-1]).values
            action = plot_data["action"].map(lambda x: x[0]).values

            label = f"Max Speed: {max_speed_i}"

            ax[1].plot(x_values, angle, label=label)

            ax[2].plot(x_values, angular_velocity, label=label)

    if not dry_run:
        x_values = np.array(max_speed)
        y_values = np.array(results)

        ax[0].plot(x_values, y_values)
        ax[0].scatter(x_values, y_values)

        ax[0].set_xlabel("Max Speed")
        ax[0].set_ylabel("Number of Finished Runs")
        ax[0].set_title("Number of Pendulum simulations finished")

        ax[1].set_xlabel("Step")
        ax[1].set_ylabel("Angle")
        ax[1].set_title(f"Pendulum angle progression")
        ax[1].legend()

        ax[2].set_xlabel("Step")
        ax[2].set_ylabel("Angular Velocity")
        ax[2].axhline(
            constraint_threshold,
            color="red",
            linestyle="--",
            label="Original Max Velocity",
        )
        ax[2].axhline(-constraint_threshold, color="red", linestyle="--")
        ax[2].set_title(f"Angular velocity progression")
        ax[2].legend()

        ext = ".png"
        plot_name = generate_file_name(plot_dir, plot_max_speed_prefix, ext) + ext
        plot_path = os.path.join(plot_dir, plot_name)
        plt.savefig(plot_path)

    return


"""
Plotting and Data Storage
"""


def plot_trajectory(trajectory: pd.DataFrame, savefig=False):
    x_values = trajectory["step"].values
    angular_velocity = trajectory["obs"].map(lambda x: x[-1]).values
    action = trajectory["action"].map(lambda x: x[0]).values

    fig, ax = plt.subplots(2, 1, figsize=(10, 8))
    ax[0].plot(x_values, angular_velocity, label="Observed Angular Velocity")
    ax[0].set_xlabel("Step")
    ax[0].set_ylabel("Angular Velocity")
    ax[0].axhline(
        constraint_threshold, color="red", linestyle="--", label="Original Max Velocity"
    )
    ax[0].axhline(-constraint_threshold, color="red", linestyle="--")
    ax[0].legend()

    ax[1].plot(x_values, action, label="Action")
    ax[1].set_xlabel("Step")
    ax[1].set_ylabel("Action")

    ext = ".png"
    plot_name = generate_file_name(plot_dir, plot_prefix, ext) + ext
    plot_path = os.path.join(plot_dir, plot_name)

    plt.suptitle("Constraint Violation Trajectory")
    plt.savefig(plot_path)

    return


def comparison_plot(
    labeled_trajectories: List[Tuple[str, pd.DataFrame]], savefig=False
):
    fig, ax = plt.subplots(2, 1, figsize=(10, 8))

    ax[0].set_xlabel("Step")
    ax[0].set_ylabel("Angular Velocity")
    ax[0].axhline(
        constraint_threshold, color="red", linestyle="--", label="Original Max Velocity"
    )
    ax[0].axhline(-constraint_threshold, color="red", linestyle="--")

    ax[1].set_xlabel("Step")
    ax[1].set_ylabel("Action")

    for i in range(len(labeled_trajectories)):
        policy_name, trajectory = labeled_trajectories[i]
        x_values = trajectory["step"].values
        angular_velocity = trajectory["obs"].map(lambda x: x[-1]).values
        action = trajectory["action"].map(lambda x: x[0]).values

        ax[0].plot(
            x_values,
            angular_velocity,
            label=f"Observed Velocity ({policy_name})",
        )

        ax[1].plot(x_values, action, label=f"Action ({policy_name})")

    ax[0].legend()
    ax[1].legend()

    ext = ".png"
    plot_name = generate_file_name(plot_dir, plot_comparison_prefix, ext) + ext
    plot_path = os.path.join(plot_dir, plot_name)

    plt.suptitle("Constraint Violation Comparison")
    plt.savefig(plot_path)

    return plot_name


def store_data(df: pd.DataFrame, prefix: str):
    ext = ".csv"
    file_name = generate_file_name(data_dir, prefix, ext) + ext
    file_path = os.path.join(data_dir, file_name)
    df.to_csv(file_path, index=False)

    return file_name


"""
Helper Functions
"""


def generate_file_name(directory, prefix, ext):
    """
    Find the next available prefix with a number that does not exist in the directory.

    Args:
    - directory (str): The directory to search for files.
    - prefix (str): The prefix to check for.

    Returns:
    - str: The next available prefix with a unique number.
    """
    existing_files = [file for file in os.listdir(directory) if file.startswith(prefix)]

    # Extract existing numbers from the filenames
    existing_numbers = set()
    for file in existing_files:
        try:
            extracted_name = file.replace(ext, "")
            number = int(extracted_name.split("-")[-1])
            existing_numbers.add(number)
        except ValueError:
            pass

    # Find the next available number
    next_number = 1
    while next_number in existing_numbers:
        next_number += 1

    next_prefix = f"{prefix}-{next_number}"
    return next_prefix


def generate_initial_states(env, num_runs: int, seed: int = 38147, any: bool = False):
    _, _ = env.reset(seed=seed)
    if any:
        init_obs = [env.reset_any()[0] for _ in range(num_runs)]
    else:
        init_obs = [env.reset()[0] for _ in range(num_runs)]

    init_states = [np.array([np.arctan2(obs[1], obs[0]), obs[2]]) for obs in init_obs]
    # init_states = [np.array([np.arctan2(obs[1], obs[0]), 4]) for obs in init_obs]
    return init_states


def create_cem_planner(config: Dict):
    # Create the optimizer
    optimizer = CrossEntropyMethod(**config)
    dynamical_system = PendulumSystem()
    cem_planner = CEMPlanner(dynamical_system, optimizer, num_particles)

    return cem_planner


def create_safe_planner(config: Dict):
    # Create the optimizer
    optimizer = CrossEntropyMethod(**config)
    cost_model = PendulumCost(max_speed=constraint_threshold)
    dynamical_system = SafePendulumSystem(cost=cost_model)
    safe_cem_planner = SafeCEMPlanner(dynamical_system, optimizer, num_particles)

    return safe_cem_planner


def create_min_max_planner(action_config: Dict, hal_action_config: Dict):
    # Create the optimizer
    x_consts = OptVarConstants(**action_config)
    y_consts = OptVarConstants(**hal_action_config)
    var_x = OptVarParams(x_consts)
    var_y = OptVarParams(y_consts)
    optimizer = MinMaxOptimizer(var_x, var_y)
    cost_model = PendulumCost(max_speed=constraint_threshold)

    dynamical_system = SafePendulumSystem(cost=cost_model)
    min_max_planner = MinMaxPlanner(
        dynamical_system, optimizer, num_particles, alpha, iterations
    )

    return min_max_planner


if __name__ == "__main__":
    # Environment parameters
    original_max_speed = 8.0
    solvable_max_speed = 5.5  # Constraint threshold
    max_speed = 12.0
    max_torque = 2.0
    num_steps = 200  # Number of environment steps
    alpha = 1.0

    num_particles = 1
    iterations = 5  # MinMax iterations

    # Test parameters
    num_runs = 10  # Number of check_constraint runs
    dry_run = False  # Whether to log the data and plots of the checks
    seed = 1945

    # Optimization function parameters
    d = 0.0
    lmbda = 100.0

    # CEM parameters
    config = {
        "action_dim": (1,),
        "horizon": 20,
        "num_elites": 50,
        "num_iter": 1,
        "num_samples": 500,
        "lower_bound": -1,
        "upper_bound": 1,
    }

    # MinMaxOptimizer configuration
    action_config = {
        "action_dim": (20, 1),  # Regular action dimension
        # "horizon": horizon,
        "num_fixed_elites": 5,
        "num_elites": 50,
        "num_iter": 3,
        "num_samples": 500,
        "lower_bound": -1,
        "upper_bound": 1,
        "minimum": True,  # Maximize the reward
    }

    hal_action_config = {
        "action_dim": (20, 3),  # Hallucinated action dimension
        # "horizon": horizon,
        "num_fixed_elites": 5,
        "num_elites": 50,
        "num_iter": 3,
        "num_samples": 500,
        "lower_bound": -1,
        "upper_bound": 1,
        "minimum": True,  # Minimize the cost
    }

    constraint_threshold = solvable_max_speed
    constraint = lambda x: x[-1] > constraint_threshold or x[-1] < -constraint_threshold

    # Create planner
    cem_planner = create_cem_planner(config)
    safe_cem_planner = create_safe_planner(config)
    min_max_planner = create_min_max_planner(action_config, hal_action_config)

    # Initialize the optimization function
    cem_optimize_fn = mean_reward
    safe_cem_optimize_fn = lambda reward, cost: relu_augmented_lagragian(
        reward, cost, d=d, lmbda=lmbda
    )

    rng = jax.random.PRNGKey(seed)

    # Define the policies to test
    custom_cem_policy = lambda obs, rng: cem_policy(
        obs, cem_planner, cem_optimize_fn, rng
    )
    custom_safe_cem_policy = lambda obs, rng: safe_cem_policy(
        obs, safe_cem_planner, safe_cem_optimize_fn, rng
    )
    custom_min_max_policy = lambda obs, rng: min_max_policy(
        obs, min_max_planner, safe_cem_optimize_fn, rng
    )

    # Create the environment
    env = ConstrainedPendulum(max_torque=max_torque, max_speed=solvable_max_speed)
    constrained_env = ConstrainedPendulum(max_torque=max_torque, max_speed=max_speed)

    # Define the experiment setups
    cem_setup = Setup(env, custom_cem_policy, "CEM")
    safe_cem_setup = Setup(constrained_env, custom_safe_cem_policy, "Safe CEM")
    min_max_setup = Setup(constrained_env, custom_min_max_policy, "MinMax")

    # Run the tests

    # run_test(env, custom_cem_policy, dry_run=dry_run, num_runs=num_runs, key=rng)
    run_test(env, custom_safe_cem_policy, dry_run=dry_run, num_runs=num_runs, key=rng)
    # run_test(env, custom_min_max_policy, dry_run=dry_run, num_runs=num_runs, key=rng)
    # comparative_test(
    #     [cem_setup, safe_cem_setup],
    #     dry_run=dry_run,
    #     num_runs=num_runs,
    #     rng=rng,
    # )

    # CEM parameters
    # default_config = {
    #     "action_dim": (1,),
    #     "horizon": 20,
    #     "num_elites": 100,
    #     "num_iter": 3,
    #     "num_samples": 2000,
    #     "lower_bound": -2,
    #     "upper_bound": 2,
    # }
    # horizon_list = [20, 30, 40, 50, 60, 70]
    # solvable_horizon_test(
    #     default_config=config, horizon_list=horizon_list, rng=rng, dry_run=dry_run
    # )

    # simultaneous_run([safe_cem_setup, min_max_setup], key=rng, dry_run=dry_run)

    # lmbda_list = [0.01, 0.1, 1.0, 10.0, 100.0]
    # d = 0.0
    # test_parameters(
    #     lmbda=lmbda_list,
    #     d=d,
    #     dry_run=dry_run,
    #     num_runs=num_runs,
    # )

    # max_speed_list = np.arange(5.0, 6.0, 0.1).tolist()
    max_speed_list = [5.5, 8.0]
    test_max_speed_threshold(
        env, max_speed=max_speed_list, dry_run=dry_run, num_runs=num_runs
    )
    print("Done.")
